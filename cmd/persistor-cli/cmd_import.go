package main

import (
	"context"
	"database/sql"
	"fmt"
	"os"
	"path/filepath"
	"sort"
	"strings"

	"github.com/persistorai/persistor/client"
	"github.com/spf13/cobra"

	_ "github.com/mattn/go-sqlite3"
)

func newImportCmd() *cobra.Command {
	cmd := &cobra.Command{
		Use:   "import",
		Short: "Import data from external sources",
	}
	cmd.AddCommand(newImportOpenClawCmd())
	return cmd
}

func newImportOpenClawCmd() *cobra.Command {
	var (
		skipEmbeddings bool
		dryRun         bool
		batchSize      int
	)

	cmd := &cobra.Command{
		Use:   "openclaw-memory <path-to-sqlite>",
		Short: "Import OpenClaw memory chunks into the knowledge graph",
		Long: `Reads an OpenClaw memory SQLite database (chunks table) and creates
a node per chunk with file path, line range, and source as properties.
Auto-creates edges between sequential chunks from the same file.

Embeddings are always re-generated by Persistor (OpenClaw uses incompatible
embedding models/dimensions). Use --skip-embeddings to import without vectors
and queue them for async generation via 'persistor admin backfill-embeddings'.`,
		Args: cobra.ExactArgs(1),
		RunE: func(cmd *cobra.Command, args []string) error {
			return runImportOpenClaw(args[0], skipEmbeddings, dryRun, batchSize)
		},
	}

	cmd.Flags().BoolVar(&skipEmbeddings, "skip-embeddings", false,
		"Import without vectors; queue for async embed worker")
	cmd.Flags().BoolVar(&dryRun, "dry-run", false,
		"Show what would be imported without making changes")
	cmd.Flags().IntVar(&batchSize, "batch-size", 100,
		"Number of nodes per bulk upsert batch")

	return cmd
}

// openclawChunk mirrors the OpenClaw chunks table schema.
type openclawChunk struct {
	ID        string
	Path      string
	Source    string
	StartLine int
	EndLine   int
	Hash      string
	Model     string
	Text      string
}

func runImportOpenClaw(dbPath string, skipEmbeddings, dryRun bool, batchSize int) error {
	// Validate path
	absPath, err := filepath.Abs(dbPath)
	if err != nil {
		return fmt.Errorf("resolve path: %w", err)
	}
	if _, err := os.Stat(absPath); err != nil {
		return fmt.Errorf("sqlite file not found: %w", err)
	}

	// Open SQLite
	db, err := sql.Open("sqlite3", absPath+"?mode=ro")
	if err != nil {
		return fmt.Errorf("open sqlite: %w", err)
	}
	defer db.Close()

	// Read all chunks
	rows, err := db.Query(`SELECT id, path, source, start_line, end_line, hash, model, text FROM chunks ORDER BY path, start_line`)
	if err != nil {
		return fmt.Errorf("query chunks: %w", err)
	}
	defer rows.Close()

	var chunks []openclawChunk
	for rows.Next() {
		var c openclawChunk
		if err := rows.Scan(&c.ID, &c.Path, &c.Source, &c.StartLine, &c.EndLine, &c.Hash, &c.Model, &c.Text); err != nil {
			return fmt.Errorf("scan chunk: %w", err)
		}
		chunks = append(chunks, c)
	}
	if err := rows.Err(); err != nil {
		return fmt.Errorf("iterate chunks: %w", err)
	}

	fmt.Fprintf(os.Stderr, "Found %d chunks in %s\n", len(chunks), filepath.Base(dbPath))

	if len(chunks) == 0 {
		fmt.Fprintln(os.Stderr, "Nothing to import.")
		return nil
	}

	// Build node requests
	nodeReqs := make([]client.CreateNodeRequest, 0, len(chunks))
	for _, c := range chunks {
		label := truncateLabel(c.Path, c.StartLine, c.EndLine)
		nodeReqs = append(nodeReqs, client.CreateNodeRequest{
			ID:    "openclaw-" + c.ID[:minInt(12, len(c.ID))],
			Type:  "memory_chunk",
			Label: label,
			Properties: map[string]any{
				"file_path":      c.Path,
				"source":         c.Source,
				"start_line":     c.StartLine,
				"end_line":       c.EndLine,
				"content_hash":   c.Hash,
				"embedding_model": c.Model,
				"text":           c.Text,
				"import_source":  "openclaw-memory",
			},
		})
	}

	// Build edges: sequential chunks in the same file
	// Group chunks by path, then create NEXT_CHUNK edges
	byPath := make(map[string][]int) // path -> indices into nodeReqs
	for i, c := range chunks {
		byPath[c.Path] = append(byPath[c.Path], i)
	}

	var edgeReqs []client.CreateEdgeRequest
	for path, indices := range byPath {
		// Sort by start_line (should already be sorted from query, but be safe)
		sort.Slice(indices, func(a, b int) bool {
			return chunks[indices[a]].StartLine < chunks[indices[b]].StartLine
		})
		for j := 0; j < len(indices)-1; j++ {
			srcIdx := indices[j]
			tgtIdx := indices[j+1]
			edgeReqs = append(edgeReqs, client.CreateEdgeRequest{
				Source:   nodeReqs[srcIdx].ID,
				Target:   nodeReqs[tgtIdx].ID,
				Relation: "NEXT_CHUNK",
				Properties: map[string]any{
					"file_path": path,
				},
			})
		}
	}

	fmt.Fprintf(os.Stderr, "Prepared: %d nodes, %d edges (%d files)\n",
		len(nodeReqs), len(edgeReqs), len(byPath))

	if dryRun {
		fmt.Fprintln(os.Stderr, "\n[DRY RUN] No changes made.")
		// Show summary by file
		type fileSummary struct {
			path   string
			count  int
		}
		var summaries []fileSummary
		for path, indices := range byPath {
			summaries = append(summaries, fileSummary{path, len(indices)})
		}
		sort.Slice(summaries, func(i, j int) bool {
			return summaries[i].path < summaries[j].path
		})
		headers := []string{"FILE", "CHUNKS"}
		var rows [][]string
		for _, s := range summaries {
			rows = append(rows, []string{s.path, fmt.Sprintf("%d", s.count)})
		}
		formatTable(headers, rows)
		return nil
	}

	ctx := context.Background()

	// Bulk upsert nodes in batches
	totalNodes := 0
	for i := 0; i < len(nodeReqs); i += batchSize {
		end := i + batchSize
		if end > len(nodeReqs) {
			end = len(nodeReqs)
		}
		batch := nodeReqs[i:end]
		nodes, err := apiClient.Bulk.UpsertNodes(ctx, batch)
		if err != nil {
			return fmt.Errorf("bulk upsert nodes (batch %d-%d): %w", i, end-1, err)
		}
		totalNodes += len(nodes)
		fmt.Fprintf(os.Stderr, "  nodes: %d/%d\r", totalNodes, len(nodeReqs))
	}
	fmt.Fprintf(os.Stderr, "  nodes: %d/%d ✓\n", totalNodes, len(nodeReqs))

	// Bulk upsert edges in batches
	totalEdges := 0
	for i := 0; i < len(edgeReqs); i += batchSize {
		end := i + batchSize
		if end > len(edgeReqs) {
			end = len(edgeReqs)
		}
		batch := edgeReqs[i:end]
		edges, err := apiClient.Bulk.UpsertEdges(ctx, batch)
		if err != nil {
			return fmt.Errorf("bulk upsert edges (batch %d-%d): %w", i, end-1, err)
		}
		totalEdges += len(edges)
		fmt.Fprintf(os.Stderr, "  edges: %d/%d\r", totalEdges, len(edgeReqs))
	}
	if len(edgeReqs) > 0 {
		fmt.Fprintf(os.Stderr, "  edges: %d/%d ✓\n", totalEdges, len(edgeReqs))
	}

	// Queue embedding backfill if not skipping
	if !skipEmbeddings {
		queued, err := apiClient.Admin.BackfillEmbeddings(ctx)
		if err != nil {
			fmt.Fprintf(os.Stderr, "Warning: backfill-embeddings failed: %v\n", err)
			fmt.Fprintln(os.Stderr, "Run 'persistor admin backfill-embeddings' manually.")
		} else if queued > 0 {
			fmt.Fprintf(os.Stderr, "  embeddings queued: %d (will process in background)\n", queued)
		}
	} else {
		fmt.Fprintln(os.Stderr, "  embeddings: skipped (run 'persistor admin backfill-embeddings' when ready)")
	}

	fmt.Fprintf(os.Stderr, "\nImport complete: %d nodes, %d edges\n", totalNodes, totalEdges)

	result := map[string]any{
		"nodes_imported": totalNodes,
		"edges_created":  totalEdges,
		"files":          len(byPath),
		"source":         dbPath,
	}
	output(result, fmt.Sprintf("%d", totalNodes))
	return nil
}

func truncateLabel(path string, startLine, endLine int) string {
	// Use filename + line range as the label
	base := filepath.Base(path)
	dir := filepath.Dir(path)
	if dir != "." {
		// Include parent dir for context
		parts := strings.Split(dir, "/")
		if len(parts) > 0 {
			base = parts[len(parts)-1] + "/" + base
		}
	}
	return fmt.Sprintf("%s L%d-%d", base, startLine, endLine)
}

func minInt(a, b int) int {
	if a < b {
		return a
	}
	return b
}
